{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f76ffd-b0c0-410d-9d0e-f84fb830edd0",
   "metadata": {},
   "source": [
    "# Requests et BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daec809-f90b-4284-a372-1e4218fe949c",
   "metadata": {},
   "source": [
    "## Installation Des Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0799ce20-19e8-4225-aa45-aefa482222ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si nous n'avez pas encore installé ces packages il faut enlever les les lignes suivantes en commentaire etles executer\n",
    "# !pip install requests --upgrade --quiet\n",
    "# !pip install beautifulsoup4 --upgrade --quiet\n",
    "# !pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0970d7-e956-44d1-b517-f3bb927e2efd",
   "metadata": {},
   "source": [
    "## Importation des Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f456c4-ce79-43e1-868d-8ac1737c10ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fda70c2-716d-4f81-aee4-6098a755c851",
   "metadata": {},
   "source": [
    "## Comment sa fonctionne\n",
    "\n",
    "La première étape consiste à utiliser le package <code>**Requests**</code> pour télécharger le code HTML de notre page Web (à l'aide de la fonction <code>**requests.get**</code>) qui donne un objet de réponse. Nous pouvons voir si nous pouvons télécharger la page Web ou non en vérifiant la réponse. Statut de notre objet. (Il devrait être compris entre <code>**200 et 299**</code>). Ensuite, nous allons convertir cet objet de réponse en un objet <code>**BeautifulSoup**</code> en utilisant le constructeur <code>**BeautifulSoup()**</code>. Nous utiliserons cet objet pour inspecter notre document et extraire les données souhaitées. <code>**BeautifulSoup**</code> nécessite un argument supplémentaire appelé <code>**parser**</code>. (En bref, **BeautifulSoup** peut également être utilisé pour extraire des informations d'autres langages de balisage. Le **parser** par défaut est <code>**\"html.parser\"**</code>). Trouvez ci-dessous une fonction d'exemple indépendante qui effectuera la tâche ci-dessus: \n",
    "* de téléchargement des pages, \n",
    "* de vérification des réponses\n",
    "d'analyse à l'aide de <code>**BeautifulSoup()**</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5000412-4736-423b-859c-265e89114a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_page(items_url):\n",
    "    # télécharger la page\n",
    "    response = requests.get(items_url)\n",
    "    # vérifier le succès de réponse\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(items_url))\n",
    "    # Parser la réponse à l'aide de beaufifulSoup\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7519f3-a43d-4443-90fb-fcafcb1e6b28",
   "metadata": {},
   "source": [
    "## Scraper la liste des articles de notre site Web cible\n",
    "Dans notre cas, nous allons scraper https://www.alibaba.com/trade/search?fsb=y&IndexArea=product_en&CatId=&SearchText=Inflight+Items&viewtype=G&tab={page} Nous obtiendrons une liste des produits en vol. Nous obtiendrons un **titre d'article principal**, un **prix**, **des notes** et une **URL** de page pour chaque article. Pour **chaque article**, nous obtiendrons plus de détails sur le produit à partir de la page de l'article (si nécessaire). Nous allons créer un dataframe de Pandas et un fichier **Json** avec les détails de l'élément. Pour chaque article, nous créerons également un fichier **CSV** au format indicatif suivant:\n",
    "\n",
    "nom, prix, URL\n",
    "Article JUNIO Fleur Préservée Avec Boîtes, 5.50 - 6.50, https://www.alibaba.com/product-detail/Item-JUNIO-Preserved-Flower-With-Boxes_62339028105.html?s=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f2ec73-b4a9-417c-b07f-26278a9a8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.alibaba.com/trade/search?fsb=y&IndexArea=product_en&CatId=&SearchText=Inflight+Items&viewtype=G&tab=%7Bpage%7D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2469970-b188-46fc-a376-3d0ac479273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_item_page(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1f2f41-cb1c-4471-a9fb-d0a5228e754f",
   "metadata": {},
   "source": [
    "## Comprendre les bases de BeautifulSoup\n",
    "\n",
    "**BeautifulSoup** a plusieurs méthodes pour extraire des informations de notre document analysé. La façon la plus simple de rechercher une balise dans notre document est d'appeler directement le nom de la balise que nous recherchons. Par exemple, si je veux la balise &lt;title&gt;, j'appellerai <code>doc.title</code>. Si je passe <code>doc.a</code>, il renverra l'occurrence de la première balise &lt;a&gt; présente dans mon document. Et si je voulais toutes les balises &lt;a&gt; ? Nous devons utiliser : « La méthode <code>find_all()</code> »."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18903233-6075-4ee7-af5a-9a92a87829bf",
   "metadata": {},
   "source": [
    "## BeautifulSoup: (.find_all() method)\n",
    "\n",
    "><code>doc.find_all(tag_name, attributes, limit, string, recursive=True)</code>:\n",
    "\n",
    "* tag_name: Le nom des éléments **HTML** ex: &lt;a&gt;, &lt;tr&gt;\n",
    "* attributes: un dict contenant les attributs d'une balise ex: {“class”: “abcabc”}\n",
    "* limit: un nombre pour limiter le nombre de balises sous cette correspondance.\n",
    "* text: une chaîne pour correspondre au contenu d'un élément \n",
    "* recursive: Par défaut, Beautiful Soup recherche tous les éléments enfants. Ainsi, le paramètre recursive = False limitera la recherche au premier élément trouvé et à son enfant uniquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca78a3ce-b5a7-427f-b97c-9810a827af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tousLesDiv = doc.find_all('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ee85e2f-16e4-4d5e-a175-cdeed0f43789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tousLesDiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a56d7863-8a44-442e-89df-310b901e4ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tousLesDivAvecClass = doc.find_all('div',{'class': 'sc-hd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "831ca8fc-443f-4507-bce1-96b195534181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tousLesDivAvecClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37a01bbd-bc76-4f14-bfad-e76f36ce7155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sixPremierDiv = doc.find_all('div', limit=6)\n",
    "len(sixPremierDiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e79cba6-9de5-480b-8da0-7f0c6624afb2",
   "metadata": {},
   "source": [
    "## BeautifulSoup: (.find() method)\n",
    "\n",
    "Une autre méthode répandue est, qui fonctionne comme mais ne renvoie que la première occurrence de la balise que nous avons demandée\n",
    "\n",
    "Dans notre recherche spécifique, nous avons des attributs dont la valeur est nulle. Donc, tout d'abord, nous organisons/analysons les données par rapport à la balise de liste d'éléments. En recherchant sur le site Web à l'aide de l'option Inspecter (clic droit sur les éléments), nous avons constaté que les informations sont disposées dans la balise div avec class = \"organic-gallery-offer-outter J-offer-wrapper\".\n",
    "\n",
    "![image](images/1.png)\n",
    "\n",
    "Le code du Web Scraping dépend de la structure de la page Web. Donc, si la conception change, votre code a également besoin d'une mise à jour. Nous utilisons une boucle for et une méthode append pour organiser les données en item_list_tags. Nous utilisons également une combinaison de [**sleep()**](https://www.programiz.com/python-programming/time/sleep, 'doc de sleep') et de [**randint**](https://www.geeksforgeeks.org/python-randint-function/, 'doc de randint') pour être doux sur le site Web. (La fonction randint() choisira un entier aléatoire entre les limites supérieures et inférieures données, dans ce cas, **10** et **2**, respectivement, pour chaque itération de la boucle. Utilisation de la fonction randint() en combinaison avec la fonction **sleep()** aidera à ajouter des pauses courtes et aléatoires dans la vitesse d'exploration du programme. La fonction sleep() arrêtera l'exécution du programme pendant le nombre de secondes donné. Le nombre de secondes sera introduit de manière aléatoire dans la fonction sleep à l'aide de randint( ). Reportez-vous au code ci-dessous pour faciliter la compréhension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "06bfdd54-5623-4efc-856c-25ff8a8c13f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement du nombre de page 1\n",
      "Téléchargement du nombre de page 2\n",
      "Téléchargement du nombre de page 3\n",
      "Téléchargement du nombre de page 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maintenant nous avons un résumé au dessus de la fonction \n",
    "def get_item_list_tags():\n",
    "    item_list_tags = []\n",
    "    for page in range(1,5):\n",
    "        items_url = f\"https://www.alibaba.com/trade/search?fsb=y&IndexArea=product_en&CatId=&SearchText=Inflight+Items&viewtype=G&tab={page}\"\n",
    "        response = requests.get(items_url)\n",
    "        page_contents = response.text\n",
    "        if response.status_code != 200:\n",
    "            raise Exception('Failed to load page {}'.format(items_url))\n",
    "        doc = BeautifulSoup(page_contents, \"html.parser\")\n",
    "        for item in doc.find_all(\"div\", {'class': \"organic-gallery-offer-outter J-offer-wrapper\"}):\n",
    "                item_list_tags.append(item)\n",
    "        sleep(randint(2,10))\n",
    "        print('Téléchargement du nombre de page', page)  \n",
    "    return item_list_tags\n",
    "item_list_tags = get_item_list_tags()\n",
    "len (item_list_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95503472-6c60-4a1a-abcb-b3417dd32878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
